# An oversight in using pre-trained models

Currently I am in the middle of the "CIFAKE Real and AI-Generated Synthetic Images" challenge on kaggle. I have changed a number of different parameters including 
how the data is augmented and the training/test ratio. However, something I only now dicovered is the possible need to upscale my dataset images from 32x32 pixels to
224x224. Sources online pointed to the research paper 'Deep Residual Learning for Image Recognition' (linked below) as to why. A lot of the popular image 
classification models offered through 'timm', such as resnet and densenet were trained on images of 224x224, making them particularly proficient at identifying 
features from images with those dimensions. Currently experimentation in this challenge has found that there has been difficulty surpassing the ~95% accuracy 
threshold, perhaps this is the solution. 

https://arxiv.org/abs/1512.03385
